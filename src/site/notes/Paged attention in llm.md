---
{"dg-publish":true,"permalink":"/paged-attention-in-llm/","tags":["compilation"]}
---

# paged attention in llm

A way to cache llm response
Normal kv cache is not applicable for llm response

https://youtu.be/1RxOYLa69Vw

